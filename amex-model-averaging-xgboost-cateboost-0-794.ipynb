{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/erwanchesneau/amex-model-averaging-xgboost-cateboost-0-794?scriptVersionId=104558227\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# LOAD LIBRARIES\nimport os\nimport gc\nimport pickle\nimport pandas as pd\nimport numpy as np # CPU libraries\nimport cupy, cudf # GPU libraries\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom catboost import Pool, CatBoostClassifier\ntry : \n    from pytorch_widedeep.preprocessing import TabPreprocessor\n    from pytorch_widedeep.models import TabTransformer, WideDeep\nexcept ModuleNotFoundError :\n    !pip install pytorch_widedeep\n    from pytorch_widedeep.preprocessing import TabPreprocessor\n    from pytorch_widedeep.models import TabTransformer, WideDeep\nprint('RAPIDS version',cudf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:19:54.947813Z","iopub.execute_input":"2022-08-29T15:19:54.948273Z","iopub.status.idle":"2022-08-29T15:20:08.635668Z","shell.execute_reply.started":"2022-08-29T15:19:54.948183Z","shell.execute_reply":"2022-08-29T15:20:08.634429Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"RAPIDS version 21.10.01\n","output_type":"stream"}]},{"cell_type":"code","source":"# VERSION NAME FOR SAVED MODEL FILES\nVER = 2\n\n# TRAIN RANDOM SEED\nSEED = 42\n\n# FILL NAN VALUE\nNAN_VALUE = -127 # will fit in int8\n\n# FOLDS PER MODEL\nFOLDS = 10\n\nTRAIN_PATH = '../input/amex-data-integer-dtypes-parquet-format/train.parquet'\n\nODIR = \"/kaggle/working/echesneau/\"\nif not os.path.isdir(ODIR):\n    os.makedirs(ODIR)\n\nTRAIN_SUBSAMPLE = 1.0\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:08.640971Z","iopub.execute_input":"2022-08-29T15:20:08.644191Z","iopub.status.idle":"2022-08-29T15:20:08.654002Z","shell.execute_reply.started":"2022-08-29T15:20:08.644145Z","shell.execute_reply":"2022-08-29T15:20:08.652999Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def read_file(path = '', usecols = None):\n    \"\"\"\n    function to load dataset\n    The function is modified frm the original one\n    The Fillna is done only during the processing\n    \"\"\"\n    # LOAD DATAFRAME\n    if usecols is not None:\n        data = cudf.read_parquet(path, columns=usecols)\n    else:\n        data = cudf.read_parquet(path)\n    # REDUCE DTYPE FOR CUSTOMER AND DATE\n    data['customer_ID'] = data['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    data.S_2 = cudf.to_datetime( data.S_2 )\n    print('shape of data:', data.shape)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:08.659101Z","iopub.execute_input":"2022-08-29T15:20:08.659894Z","iopub.status.idle":"2022-08-29T15:20:08.666903Z","shell.execute_reply.started":"2022-08-29T15:20:08.659856Z","shell.execute_reply":"2022-08-29T15:20:08.665923Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def process_and_feature_engineer(data):\n    \"\"\"\n    function to process database\n    FEATURE ENGINEERING FROM\n    https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n    \"\"\"\n    all_cols = [c for c in list(data.columns) if c not in ['customer_ID','S_2']]\n    cat_feat = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\\\n                    \"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n    num_features = [col for col in all_cols if col not in cat_feat]\n\n    test_num_agg = data.groupby(\"customer_ID\")[num_features].agg(['mean', \\\n                                                                  'std', \\\n                                                                  'min', \\\n                                                                  'max', \\\n                                                                  'last'])\n    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n\n    test_cat_agg = data.groupby(\"customer_ID\")[cat_feat].agg(['count', \\\n                                                              'last', \\\n                                                              'nunique'])\n    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n\n    data = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n    del test_num_agg, test_cat_agg\n    data = data.fillna(NAN_VALUE)\n    print('shape after engineering', data.shape )\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:08.670409Z","iopub.execute_input":"2022-08-29T15:20:08.671231Z","iopub.status.idle":"2022-08-29T15:20:08.681466Z","shell.execute_reply.started":"2022-08-29T15:20:08.671189Z","shell.execute_reply":"2022-08-29T15:20:08.680511Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def amex_metric_mod(y_true, y_pred):\n    \"\"\"\n    function to calculate the metric of the competion\n    from https://www.kaggle.com/kyakovlev\n    and https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\n    \"\"\"\n    labels     = np.transpose(np.array([y_true, y_pred]))\n    labels     = labels[labels[:, 1].argsort()[::-1]]\n    weights    = np.where(labels[:,0]==0, 20, 1)\n    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n\n    gini = [0,0]\n    for i in [1,0]:\n        labels         = np.transpose(np.array([y_true, y_pred]))\n        labels         = labels[labels[:, i].argsort()[::-1]]\n        weight         = np.where(labels[:,0]==0, 20, 1)\n        weight_random  = np.cumsum(weight / np.sum(weight))\n        total_pos      = np.sum(labels[:, 0] *  weight)\n        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n        lorentz        = cum_pos_found / total_pos\n        gini[i]        = np.sum((lorentz - weight_random) * weight)\n\n    return 0.5 * (gini[1]/gini[0] + top_four)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:08.683251Z","iopub.execute_input":"2022-08-29T15:20:08.684162Z","iopub.status.idle":"2022-08-29T15:20:08.696112Z","shell.execute_reply.started":"2022-08-29T15:20:08.684122Z","shell.execute_reply":"2022-08-29T15:20:08.695159Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"result_all = pd.DataFrame(columns=['model', 'preprocessing', 'name', 'y_valid_pred', 'y_pred','valid_acc', 'acc'])\nresult_sum = pd.DataFrame(columns=['model', 'preprocessing', 'name', 'y_pred', 'acc'])","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:08.697887Z","iopub.execute_input":"2022-08-29T15:20:08.698779Z","iopub.status.idle":"2022-08-29T15:20:08.713941Z","shell.execute_reply.started":"2022-08-29T15:20:08.698733Z","shell.execute_reply":"2022-08-29T15:20:08.71279Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# @huseyincot preprocessing","metadata":{}},{"cell_type":"markdown","source":"The processing proposed by @Huyseioncot seems to be interessting and it is one of the most used.\nSo we decide to base the predictions on this processing.","metadata":{}},{"cell_type":"markdown","source":"## Load and process","metadata":{}},{"cell_type":"markdown","source":"Parquet format is use to save GPU/RAM memory.","metadata":{}},{"cell_type":"code","source":"print('Reading train data...')\ntrain = read_file(path = TRAIN_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:08.715339Z","iopub.execute_input":"2022-08-29T15:20:08.715868Z","iopub.status.idle":"2022-08-29T15:20:34.625533Z","shell.execute_reply.started":"2022-08-29T15:20:08.715832Z","shell.execute_reply":"2022-08-29T15:20:34.623279Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Reading train data...\nshape of data: (5531451, 190)\n","output_type":"stream"}]},{"cell_type":"code","source":"train = process_and_feature_engineer(train)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:34.627074Z","iopub.execute_input":"2022-08-29T15:20:34.627816Z","iopub.status.idle":"2022-08-29T15:20:36.917698Z","shell.execute_reply.started":"2022-08-29T15:20:34.627779Z","shell.execute_reply":"2022-08-29T15:20:36.916505Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"shape after engineering (458913, 918)\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:36.920411Z","iopub.execute_input":"2022-08-29T15:20:36.920842Z","iopub.status.idle":"2022-08-29T15:20:37.591625Z","shell.execute_reply.started":"2022-08-29T15:20:36.9208Z","shell.execute_reply":"2022-08-29T15:20:37.590545Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                      P_2_mean   P_2_std   P_2_min   P_2_max  P_2_last  \\\ncustomer_ID                                                              \n-9223358381327749917  0.415868  0.057145  0.340178  0.498727  0.387708   \n-9223193039457028513  0.974068  0.013094  0.964483  1.002478  1.001372   \n-9223189665817919541  0.802447  0.038025  0.694073  0.828761  0.694073   \n-9223188534444851899  0.791203  0.002688  0.786647  0.794826  0.787945   \n-9223173911659837606  0.115666  0.078554  0.038207  0.252421  0.040486   \n\n                      D_39_mean  D_39_std  D_39_min  D_39_max  D_39_last  ...  \\\ncustomer_ID                                                               ...   \n-9223358381327749917   2.615385  4.628507         0        16          0  ...   \n-9223193039457028513   0.000000  0.000000         0         0          0  ...   \n-9223189665817919541   0.000000  0.000000         0         0          0  ...   \n-9223188534444851899   0.000000  0.000000         0         0          0  ...   \n-9223173911659837606   4.384615  6.144625         0        17         13  ...   \n\n                      D_63_nunique  D_64_count  D_64_last  D_64_nunique  \\\ncustomer_ID                                                               \n-9223358381327749917             1          13          2             1   \n-9223193039457028513             2          13          0             1   \n-9223189665817919541             1          13          0             1   \n-9223188534444851899             1          13          3             2   \n-9223173911659837606             1          13          0             2   \n\n                      D_66_count  D_66_last  D_66_nunique  D_68_count  \\\ncustomer_ID                                                             \n-9223358381327749917          13         -1             1          13   \n-9223193039457028513          13         -1             1          13   \n-9223189665817919541          13         -1             1          13   \n-9223188534444851899          13         -1             1          13   \n-9223173911659837606          13         -1             1          13   \n\n                      D_68_last  D_68_nunique  \ncustomer_ID                                    \n-9223358381327749917          3             2  \n-9223193039457028513          6             1  \n-9223189665817919541          6             1  \n-9223188534444851899          5             1  \n-9223173911659837606          6             2  \n\n[5 rows x 918 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>P_2_mean</th>\n      <th>P_2_std</th>\n      <th>P_2_min</th>\n      <th>P_2_max</th>\n      <th>P_2_last</th>\n      <th>D_39_mean</th>\n      <th>D_39_std</th>\n      <th>D_39_min</th>\n      <th>D_39_max</th>\n      <th>D_39_last</th>\n      <th>...</th>\n      <th>D_63_nunique</th>\n      <th>D_64_count</th>\n      <th>D_64_last</th>\n      <th>D_64_nunique</th>\n      <th>D_66_count</th>\n      <th>D_66_last</th>\n      <th>D_66_nunique</th>\n      <th>D_68_count</th>\n      <th>D_68_last</th>\n      <th>D_68_nunique</th>\n    </tr>\n    <tr>\n      <th>customer_ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-9223358381327749917</th>\n      <td>0.415868</td>\n      <td>0.057145</td>\n      <td>0.340178</td>\n      <td>0.498727</td>\n      <td>0.387708</td>\n      <td>2.615385</td>\n      <td>4.628507</td>\n      <td>0</td>\n      <td>16</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>13</td>\n      <td>2</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>-9223193039457028513</th>\n      <td>0.974068</td>\n      <td>0.013094</td>\n      <td>0.964483</td>\n      <td>1.002478</td>\n      <td>1.001372</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>-9223189665817919541</th>\n      <td>0.802447</td>\n      <td>0.038025</td>\n      <td>0.694073</td>\n      <td>0.828761</td>\n      <td>0.694073</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>-9223188534444851899</th>\n      <td>0.791203</td>\n      <td>0.002688</td>\n      <td>0.786647</td>\n      <td>0.794826</td>\n      <td>0.787945</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>13</td>\n      <td>3</td>\n      <td>2</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>-9223173911659837606</th>\n      <td>0.115666</td>\n      <td>0.078554</td>\n      <td>0.038207</td>\n      <td>0.252421</td>\n      <td>0.040486</td>\n      <td>4.384615</td>\n      <td>6.144625</td>\n      <td>0</td>\n      <td>17</td>\n      <td>13</td>\n      <td>...</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>2</td>\n      <td>13</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>6</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 918 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Targets are added in the database","metadata":{}},{"cell_type":"code","source":"# ADD TARGETS\ntargets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntargets = targets.set_index('customer_ID')\ntrain = train.merge(targets, left_index=True, right_index=True, how='left')\ntrain.target = train.target.astype('int8')\ndel targets\n\n# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\ntrain = train.sort_index().reset_index()\n\n# FEATURES\nFEATURES = train.columns[1:-1]\nprint(f'There are {len(FEATURES)} features!')","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:37.594891Z","iopub.execute_input":"2022-08-29T15:20:37.595231Z","iopub.status.idle":"2022-08-29T15:20:38.78175Z","shell.execute_reply.started":"2022-08-29T15:20:37.595202Z","shell.execute_reply":"2022-08-29T15:20:38.780532Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"There are 918 features!\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(ODIR+'/all_features.pkl', 'wb') as ofile :\n    pickle.dump(FEATURES, ofile)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:11:50.764987Z","iopub.execute_input":"2022-08-29T15:11:50.766001Z","iopub.status.idle":"2022-08-29T15:11:50.772106Z","shell.execute_reply.started":"2022-08-29T15:11:50.765959Z","shell.execute_reply":"2022-08-29T15:11:50.770958Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Features are needed for the prediction on the test set, \nwe save it.","metadata":{}},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"markdown","source":"XGBoost seems to be one of the most efficent model.","metadata":{}},{"cell_type":"code","source":"train = train.to_pandas() # free GPU memory\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T15:20:38.783405Z","iopub.execute_input":"2022-08-29T15:20:38.783839Z","iopub.status.idle":"2022-08-29T15:20:43.901546Z","shell.execute_reply.started":"2022-08-29T15:20:38.783797Z","shell.execute_reply":"2022-08-29T15:20:43.899896Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print('XGB Version',xgb.__version__)\n\n# XGB MODEL PARAMETERS\nxgb_parms = { \n    'max_depth':4, \n    'learning_rate':0.05, \n    'subsample':0.8,\n    'colsample_bytree':0.6, \n    'eval_metric':'logloss',\n    'objective':'binary:logistic',\n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'random_state':SEED\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-24T17:46:32.497471Z","iopub.execute_input":"2022-08-24T17:46:32.497863Z","iopub.status.idle":"2022-08-24T17:46:32.506847Z","shell.execute_reply.started":"2022-08-24T17:46:32.497827Z","shell.execute_reply":"2022-08-24T17:46:32.505877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Beause of memory limitation, database is split into flods.\nA model is train for each fold. \nAmex metric is calculated on the validation set, train set and all fold data.\nAt the end, the global metric is calculated","metadata":{}},{"cell_type":"code","source":"oof = []\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    dtrain = xgb.DMatrix(data=train.loc[train_idx, FEATURES], label=train.loc[train_idx, 'target'])\n    dvalid = xgb.DMatrix(data=train.loc[valid_idx, FEATURES], label=train.loc[valid_idx, 'target'])\n    model = xgb.train(xgb_parms, \n                      dtrain=dtrain,\n                      evals=[(dtrain,'train'),(dvalid,'valid')],\n                      num_boost_round=9999,\n                      #num_boost_round=99,\n                      early_stopping_rounds=100,\n                      verbose_eval=100) \n    model.save_model(f'{ODIR}/XGB_all_features_v{VER}_fold{fold}.xgb')\n    valid_pred = model.predict(dvalid)\n    val_acc = amex_metric_mod(train.loc[valid_idx, 'target'].values, valid_pred)\n    print('Kaggle Metric on valid set =',val_acc,'\\n')\n    \n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = valid_pred\n    oof.append( df )\n\n    del dtrain, dvalid, df\n    _ = gc.collect()\n    \n    dall = xgb.DMatrix(data=train[FEATURES], label=train['target'])\n    pred = model.predict(dall)\n    all_acc = amex_metric_mod(train['target'].values, pred)\n    print('Kaggle Metric on all dataset =',all_acc,'\\n')\n    result_all = result_all.append({'model' : \"XGBoost\", \n                                    'preprocessing' : \"huseyincot_all_feat\", \n                                    'name' : f'XGB_all_features_v{VER}_fold{fold}', \n                                    'y_valid_pred' : valid_pred, \n                                    'valid_acc' : val_acc,\n                                    'y_pred' : pred,\n                                    'acc' : all_acc\n                                   }, \n                                   ignore_index=True\n                                  )\n    del dall, pred, valid_pred\n    _ = gc.collect()\nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\nresult_sum = result_sum.append({'model' : \"XGBoost\", \n                                'preprocessing':\"huseyincot_all_feat\", \n                                'name' : \"XGBoost_huseyincot_all_feat\", \n                                'y_pred' : oof, \n                                'acc': acc\n                               },\n                               ignore_index=True\n                              )\nprint('OVERALL CV Kaggle Metric =',acc)\n\ndel oof, acc \n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T17:46:32.508592Z","iopub.execute_input":"2022-08-24T17:46:32.509005Z","iopub.status.idle":"2022-08-24T17:57:11.126021Z","shell.execute_reply.started":"2022-08-24T17:46:32.508971Z","shell.execute_reply":"2022-08-24T17:57:11.124972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost","metadata":{}},{"cell_type":"markdown","source":"CatBoost is based on the same method than XGBoost but could  be more efficient.\nWe apply the same code than before but training a catboost.","metadata":{}},{"cell_type":"code","source":"# GET CATEG VARIABLES\ncat_features = [\"B_30\", \"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\ncateg = []\n#print(train.columns)\nfor col in train.columns :\n    if col not in ['customer_ID', 'target'] :\n        var = '_'.join(col.split('_')[:2])\n        if var in cat_features :\n            categ.append(col)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:42:39.286841Z","iopub.execute_input":"2022-08-24T19:42:39.287544Z","iopub.status.idle":"2022-08-24T19:42:39.295395Z","shell.execute_reply.started":"2022-08-24T19:42:39.287495Z","shell.execute_reply":"2022-08-24T19:42:39.294233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = []\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    train_pool = Pool(train.loc[train_idx, FEATURES], \n                      train.loc[train_idx, 'target'],\n                      cat_features=categ\n                     )\n    valid_pool = Pool(train.loc[valid_idx, FEATURES], \n                      train.loc[valid_idx, 'target'],\n                      cat_features=categ\n                     )\n    model = CatBoostClassifier(iterations=9999, \n                               random_state=SEED, \n                               task_type=\"GPU\",\n                               loss_function = 'Logloss',\n                               #learning_rate=0.05\n                               )\n    model.fit(train_pool, eval_set=valid_pool,\n              #od_type=\"Iter\",\n              early_stopping_rounds=100,\n              #od_wait=100,\n              verbose=100)\n    model.save_model(f'{ODIR}/CTB_all_features_v{VER}_fold{fold}.ctb')\n    valid_pred = model.predict_proba(valid_pool)[:,1]\n    val_acc = amex_metric_mod(train.loc[valid_idx, 'target'].values, valid_pred)\n    print('Kaggle Metric on valid set =',val_acc,'\\n')\n    \n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = valid_pred\n    oof.append( df )\n\n    del train_pool, valid_pool, df\n    _ = gc.collect()\n    \n    all_pool = Pool(train[FEATURES], \n                    train['target'],\n                    cat_features=categ\n                     )\n    pred = model.predict_proba(all_pool)[:,1]\n    all_acc = amex_metric_mod(train['target'].values, pred)\n    print('Kaggle Metric on all dataset =',all_acc,'\\n')\n    result_all = result_all.append({'model' : \"CateBoost\", \n                                    'preprocessing' : \"huseyincot_all_feat\", \n                                    'name' : f'CTB_all_features_v{VER}_fold{fold}', \n                                    'y_valid_pred' : valid_pred, \n                                    'valid_acc' : val_acc,\n                                    'y_pred' : pred,\n                                    'acc' : all_acc\n                                   }, \n                                   ignore_index=True\n                                  )\n    del all_pool, pred, valid_pred\n    _ = gc.collect()\n    \nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\nresult_sum = result_sum.append({'model' : \"CateBoost\", \n                                'preprocessing':\"huseyincot_all_feat\", \n                                'name' : \"CTB_huseyincot_all_feat\", \n                                'y_pred' : oof, \n                                'acc': acc\n                               },\n                               ignore_index=True\n                              )\nprint('OVERALL CV Kaggle Metric =',acc)\n\ndel oof, acc \n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:42:39.297104Z","iopub.execute_input":"2022-08-24T19:42:39.298784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train \n_=gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing Columns with a majority of NaN","metadata":{}},{"cell_type":"markdown","source":"The EDA shows us that some features contain huge amount of NaN values.\nThese features are removed.","metadata":{}},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"train = read_file(path = TRAIN_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select features","metadata":{}},{"cell_type":"markdown","source":"Features are deleted if more than 20% of values are NaN. ","metadata":{}},{"cell_type":"code","source":"counter = train.isnull().sum(axis=0).sort_values(ascending=False)/len(train)*100\nrm_nan = counter[counter>20].index\nrm_nan = list(rm_nan.to_array())\nprint(f\"{len(rm_nan)}/{len(train.columns)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES_2 = [col for col in train.columns if col not in rm_nan]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[FEATURES_2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing","metadata":{}},{"cell_type":"markdown","source":"The same processing is applied","metadata":{}},{"cell_type":"code","source":"train = process_and_feature_engineer(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ADD TARGETS\ntargets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntargets = targets.set_index('customer_ID')\ntrain = train.merge(targets, left_index=True, right_index=True, how='left')\ntrain.target = train.target.astype('int8')\ndel targets\n\n# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\ntrain = train.sort_index().reset_index()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES_2 = train.columns[1:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(ODIR+'/all_features_2.pkl', 'wb') as ofile :\n    pickle.dump(FEATURES_2, ofile)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"train = train.to_pandas() # free GPU memory\nTRAIN_SUBSAMPLE = 1.0\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('XGB Version',xgb.__version__)\n\n# XGB MODEL PARAMETERS\nxgb_parms = { \n    'max_depth':4, \n    'learning_rate':0.05, \n    'subsample':0.8,\n    'colsample_bytree':0.6, \n    'eval_metric':'logloss',\n    'objective':'binary:logistic',\n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'random_state':SEED\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:31:52.743447Z","iopub.execute_input":"2022-08-24T18:31:52.746045Z","iopub.status.idle":"2022-08-24T18:31:52.757498Z","shell.execute_reply.started":"2022-08-24T18:31:52.745992Z","shell.execute_reply":"2022-08-24T18:31:52.755797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = []\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    dtrain = xgb.DMatrix(data=train.loc[train_idx, FEATURES_2], label=train.loc[train_idx, 'target'])\n    dvalid = xgb.DMatrix(data=train.loc[valid_idx, FEATURES_2], label=train.loc[valid_idx, 'target'])\n    model = xgb.train(xgb_parms, \n                      dtrain=dtrain,\n                      evals=[(dtrain,'train'),(dvalid,'valid')],\n                      num_boost_round=9999,\n                      #num_boost_round=99,\n                      early_stopping_rounds=100,\n                      verbose_eval=100) \n    model.save_model(f'{ODIR}/XGB_nonan_features_v{VER}_fold{fold}.xgb')\n    valid_pred = model.predict(dvalid)\n    val_acc = amex_metric_mod(train.loc[valid_idx, 'target'].values, valid_pred)\n    print('Kaggle Metric on valid set =',val_acc,'\\n')\n    \n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = valid_pred\n    oof.append( df )\n\n    del dtrain, dvalid, df\n    _ = gc.collect()\n    \n    dall = xgb.DMatrix(data=train[FEATURES_2], label=train['target'])\n    pred = model.predict(dall)\n    all_acc = amex_metric_mod(train['target'].values, pred)\n    print('Kaggle Metric on all dataset =',all_acc,'\\n')\n    #result_all = result_all.append({'model' : \"XGBoost\", \n    #                                'preprocessing' : \"huseyincot_nonan_feat\", \n    #                                'name' : f'XGB_nonan_features_v{VER}_fold{fold}', \n    #                                'y_valid_pred' : valid_pred, \n    #                                'valid_acc' : val_acc,\n    #                                'y_pred' : pred,\n    #                                'acc' : all_acc\n    #                               }, \n    #                               ignore_index=True\n    #                              )\n    del dall, pred, valid_pred\n    _ = gc.collect()\nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n#result_sum = result_sum.append({'model' : \"XGBoost\", \n#                                'preprocessing':\"huseyincot_nonan_feat\", \n#                                'name' : \"XGBoost_huseyincot_nonan_feat\", \n#                                'y_pred' : oof, \n#                                'acc': acc\n#                              },\n#                               ignore_index=True\n#                              )\nprint('OVERALL CV Kaggle Metric =',acc)\n\ndel oof, acc \n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T18:31:52.760528Z","iopub.execute_input":"2022-08-24T18:31:52.761304Z","iopub.status.idle":"2022-08-24T18:39:57.770278Z","shell.execute_reply.started":"2022-08-24T18:31:52.761259Z","shell.execute_reply":"2022-08-24T18:39:57.768478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CateBoost","metadata":{}},{"cell_type":"code","source":"# GET CATEG VARIABLES\ncat_features = [\"B_30\", \"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\ncateg = []\n#print(train.columns)\nfor col in train.columns :\n    if col not in ['customer_ID', 'target'] :\n        var = '_'.join(col.split('_')[:2])\n        if var in cat_features :\n            categ.append(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try :\n    del all_pool, pred, valid_pred\n    _ = gc.collect()\nexcept :\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = []\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    train_pool = Pool(train.loc[train_idx, FEATURES_2], \n                      train.loc[train_idx, 'target'],\n                      cat_features=categ\n                     )\n    valid_pool = Pool(train.loc[valid_idx, FEATURES_2], \n                      train.loc[valid_idx, 'target'],\n                      cat_features=categ\n                     )\n    model = CatBoostClassifier(iterations=9999, \n                               random_state=SEED, \n                               task_type=\"GPU\",\n                               loss_function = 'Logloss',\n                               #learning_rate=0.05\n                               )\n    model.fit(train_pool, eval_set=valid_pool,\n              #od_type=\"Iter\",\n              early_stopping_rounds=100,\n              #od_wait=100,\n              verbose=100)\n    model.save_model(f'{ODIR}/CTB_nonan_features_v{VER}_fold{fold}.ctb')\n    valid_pred = model.predict_proba(valid_pool)[:,1]\n    val_acc = amex_metric_mod(train.loc[valid_idx, 'target'].values, valid_pred)\n    print('Kaggle Metric on valid set =',val_acc,'\\n')\n    \n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = valid_pred\n    oof.append( df )\n\n    del train_pool, valid_pool, df\n    _ = gc.collect()\n    \n    all_pool = Pool(train[FEATURES_2], \n                    train['target'],\n                    cat_features=categ\n                     )\n    pred = model.predict_proba(all_pool)[:,1]\n    all_acc = amex_metric_mod(train['target'].values, pred)\n    print('Kaggle Metric on all dataset =',all_acc,'\\n')\n    #result_all = result_all.append({'model' : \"CateBoost\", \n    #                                'preprocessing' : \"huseyincot_nonan_feat\", \n    #                                'name' : f'CTB_nonan_features_v{VER}_fold{fold}', \n    #                                'y_valid_pred' : valid_pred, \n    #                                'valid_acc' : val_acc,\n    #                                'y_pred' : pred,\n    #                                'acc' : all_acc\n    #                               }, \n    #                               ignore_index=True\n    #                              )\n    del all_pool, pred, valid_pred\n    _ = gc.collect()\n    \nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n#result_sum = result_sum.append({'model' : \"CateBoost\", \n#                                'preprocessing':\"huseyincot_all_feat\", \n#                                'name' : \"CTB_huseyincot_all_feat\", \n#                                'y_pred' : oof, \n#                                'acc': acc\n#                               },\n#                               ignore_index=True\n#                              )\nprint('OVERALL CV Kaggle Metric =',acc)\n\ndel oof, acc \n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train \n_=gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features importances","metadata":{}},{"cell_type":"markdown","source":"A selection of most important features is done using the drop columns importances method.\nThe goal is to use only most important features for the prediction.","metadata":{}},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"train = read_file(path = TRAIN_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = process_and_feature_engineer(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ADD TARGETS\ntargets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntargets = targets.set_index('customer_ID')\ntrain = train.merge(targets, left_index=True, right_index=True, how='left')\ntrain.target = train.target.astype('int8')\ndel targets\n\n# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\ntrain = train.sort_index().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because of the important size of the train set and in order to spped up the selection,\nwe select only 1/400 of rows.","metadata":{}},{"cell_type":"code","source":"train = train.loc[range(int(len(train)/400))]\ntrain=train.to_pandas()\nprint(train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES_tmp = train.columns[1:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select features","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:12:42.781972Z","iopub.execute_input":"2022-08-24T19:12:42.782628Z","iopub.status.idle":"2022-08-24T19:12:42.790755Z","shell.execute_reply.started":"2022-08-24T19:12:42.782588Z","shell.execute_reply":"2022-08-24T19:12:42.789772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\ndef dropcol_importances(rf, X_train, y_train):\n    rf_ = deepcopy(rf)\n    rf_.random_state = 999\n    rf_.fit(X_train, y_train)\n    baseline = rf_.oob_score_\n    imp = []\n    for i, col in enumerate(X_train.columns):\n        print(f\"{i}/{len(X_train.columns)}\", end=\"\\r\")\n        X = X_train.drop(col, axis=1)\n        rf_ = deepcopy(rf)\n        rf_.random_state = 999\n        rf_.fit(X, y_train)\n        o = rf_.oob_score_\n        imp.append(baseline - o)\n    imp = np.array(imp)\n    I = pd.DataFrame(\n            data={'Feature':X_train.columns,\n                  'Importance':imp})\n    I = I.set_index('Feature')\n    I = I.sort_values('Importance', ascending=True)\n    return I","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:12:42.794201Z","iopub.execute_input":"2022-08-24T19:12:42.794561Z","iopub.status.idle":"2022-08-24T19:12:42.804078Z","shell.execute_reply.started":"2022-08-24T19:12:42.794536Z","shell.execute_reply":"2022-08-24T19:12:42.803127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(\n         n_estimators=100,\n         # better generality with 5\n         min_samples_leaf=5, \n         n_jobs=-1,\n         oob_score=True)\nrf.fit(train[FEATURES_tmp], train['target']) # rf must be pre-trained","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:12:42.805588Z","iopub.execute_input":"2022-08-24T19:12:42.806024Z","iopub.status.idle":"2022-08-24T19:12:43.567568Z","shell.execute_reply.started":"2022-08-24T19:12:42.805991Z","shell.execute_reply":"2022-08-24T19:12:43.5666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc_imp = dropcol_importances(rf,train[FEATURES_tmp] , train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:12:43.569135Z","iopub.execute_input":"2022-08-24T19:12:43.569479Z","iopub.status.idle":"2022-08-24T19:15:09.869982Z","shell.execute_reply.started":"2022-08-24T19:12:43.569444Z","shell.execute_reply":"2022-08-24T19:15:09.868658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc_imp","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:15:09.871405Z","iopub.status.idle":"2022-08-24T19:15:09.872246Z","shell.execute_reply.started":"2022-08-24T19:15:09.871991Z","shell.execute_reply":"2022-08-24T19:15:09.872015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The effect of each features on the accuracy is plot here","metadata":{}},{"cell_type":"code","source":"dc_imp.plot.barh()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:15:09.873572Z","iopub.status.idle":"2022-08-24T19:15:09.874292Z","shell.execute_reply.started":"2022-08-24T19:15:09.87404Z","shell.execute_reply":"2022-08-24T19:15:09.874063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only features with an importance > 0 are conserved","metadata":{}},{"cell_type":"code","source":"dc_imp[dc_imp[\"Importance\"]>0]","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:15:09.875554Z","iopub.status.idle":"2022-08-24T19:15:09.87627Z","shell.execute_reply.started":"2022-08-24T19:15:09.876019Z","shell.execute_reply":"2022-08-24T19:15:09.876042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES_3 = dc_imp[dc_imp[\"Importance\"]>0].index.to_list()\nwith open(ODIR+'/all_features_3.pkl', 'wb') as ofile :\n    pickle.dump(FEATURES_3, ofile)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:15:09.877551Z","iopub.status.idle":"2022-08-24T19:15:09.878267Z","shell.execute_reply.started":"2022-08-24T19:15:09.878014Z","shell.execute_reply":"2022-08-24T19:15:09.878038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"../input/amex-output-echesneau\"\nif os.path.isfile(MODEL_PATH+\"/all_features_3.pkl\") :\n    with open(MODEL_PATH+\"/all_features_3.pkl\", 'rb') as f :\n        FEATURES_3 = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prepare data for modelization","metadata":{}},{"cell_type":"markdown","source":"The same processing is then applY.","metadata":{}},{"cell_type":"code","source":"train = read_file(path = TRAIN_PATH)\ntrain = process_and_feature_engineer(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ADD TARGETS\ntargets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntargets = targets.set_index('customer_ID')\ntrain = train.merge(targets, left_index=True, right_index=True, how='left')\ntrain.target = train.target.astype('int8')\ndel targets\n\n# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\ntrain = train.sort_index().reset_index()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"train = train.to_pandas() # free GPU memory\nTRAIN_SUBSAMPLE = 1.0\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('XGB Version',xgb.__version__)\n\n# XGB MODEL PARAMETERS\nxgb_parms = { \n    'max_depth':4, \n    'learning_rate':0.05, \n    'subsample':0.8,\n    'colsample_bytree':0.6, \n    'eval_metric':'logloss',\n    'objective':'binary:logistic',\n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'random_state':SEED\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:16:07.485431Z","iopub.execute_input":"2022-08-24T19:16:07.485895Z","iopub.status.idle":"2022-08-24T19:16:07.492462Z","shell.execute_reply.started":"2022-08-24T19:16:07.485857Z","shell.execute_reply":"2022-08-24T19:16:07.49123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = []\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    dtrain = xgb.DMatrix(data=train.loc[train_idx, FEATURES_3], label=train.loc[train_idx, 'target'])\n    dvalid = xgb.DMatrix(data=train.loc[valid_idx, FEATURES_3], label=train.loc[valid_idx, 'target'])\n    model = xgb.train(xgb_parms, \n                      dtrain=dtrain,\n                      evals=[(dtrain,'train'),(dvalid,'valid')],\n                      num_boost_round=9999,\n                      #num_boost_round=99,\n                      early_stopping_rounds=100,\n                      verbose_eval=100) \n    model.save_model(f'{ODIR}/XGB_dc0_features_v{VER}_fold{fold}.xgb')\n    valid_pred = model.predict(dvalid)\n    val_acc = amex_metric_mod(train.loc[valid_idx, 'target'].values, valid_pred)\n    print('Kaggle Metric on valid set =',val_acc,'\\n')\n    \n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = valid_pred\n    oof.append( df )\n\n    del dtrain, dvalid, df\n    _ = gc.collect()\n    \n    dall = xgb.DMatrix(data=train[FEATURES_3], label=train['target'])\n    pred = model.predict(dall)\n    all_acc = amex_metric_mod(train['target'].values, pred)\n    print('Kaggle Metric on all dataset =',all_acc,'\\n')\n    #result_all = result_all.append({'model' : \"XGBoost\", \n    #                                'preprocessing' : \"huseyincot_dc0_feat\", \n    #                                'name' : f'XGB_dc0_features_v{VER}_fold{fold}', \n    #                                'y_valid_pred' : valid_pred, \n    #                                'valid_acc' : val_acc,\n    #                                'y_pred' : pred,\n    #                                'acc' : all_acc\n    #                               }, \n    #                               ignore_index=True\n    #                              )\n    del dall, pred, valid_pred\n    _ = gc.collect()\nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n#result_sum = result_sum.append({'model' : \"XGBoost\", \n#                                'preprocessing':\"huseyincot_dc0_feat\", \n#                                'name' : \"XGBoost_huseyincot_dc0_feat\", \n#                                'y_pred' : oof, \n#                                'acc': acc\n#                              },\n#                               ignore_index=True\n#                              )\nprint('OVERALL CV Kaggle Metric =',acc)\n\ndel oof, acc \n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:16:07.494102Z","iopub.execute_input":"2022-08-24T19:16:07.494753Z","iopub.status.idle":"2022-08-24T19:21:57.409728Z","shell.execute_reply.started":"2022-08-24T19:16:07.494701Z","shell.execute_reply":"2022-08-24T19:21:57.408765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CateBoost","metadata":{}},{"cell_type":"code","source":"# GET CATEG VARIABLES\ncat_features = [\"B_30\", \"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\ncateg = []\n#print(train.columns)\nfor col in FEATURES_3 :\n    if col not in ['customer_ID', 'target'] :\n        var = '_'.join(col.split('_')[:2])\n        if var in cat_features :\n            categ.append(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = []\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    train_pool = Pool(train.loc[train_idx, FEATURES_3], \n                      train.loc[train_idx, 'target'],\n                      cat_features=categ\n                     )\n    valid_pool = Pool(train.loc[valid_idx, FEATURES_3], \n                      train.loc[valid_idx, 'target'],\n                      cat_features=categ\n                     )\n    model = CatBoostClassifier(iterations=9999, \n                               random_state=SEED, \n                               task_type=\"GPU\",\n                               loss_function = 'Logloss',\n                               #learning_rate=0.05\n                               )\n    model.fit(train_pool, eval_set=valid_pool,\n              #od_type=\"Iter\",\n              early_stopping_rounds=100,\n              #od_wait=100,\n              verbose=100)\n    model.save_model(f'{ODIR}/CTB_dc0_features_v{VER}_fold{fold}.ctb')\n    valid_pred = model.predict_proba(valid_pool)[:,1]\n    val_acc = amex_metric_mod(train.loc[valid_idx, 'target'].values, valid_pred)\n    print('Kaggle Metric on valid set =',val_acc,'\\n')\n    \n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = valid_pred\n    oof.append( df )\n\n    del train_pool, valid_pool, df\n    _ = gc.collect()\n    \n    all_pool = Pool(train[FEATURES_3], \n                    train['target'],\n                    cat_features=categ\n                     )\n    pred = model.predict_proba(all_pool)[:,1]\n    all_acc = amex_metric_mod(train['target'].values, pred)\n    print('Kaggle Metric on all dataset =',all_acc,'\\n')\n    #result_all = result_all.append({'model' : \"CateBoost\", \n    #                                'preprocessing' : \"huseyincot_dc0_feat\", \n    #                                'name' : f'CTB_dc0_features_v{VER}_fold{fold}', \n    #                                'y_valid_pred' : valid_pred, \n    #                                'valid_acc' : val_acc,\n    #                                'y_pred' : pred,\n    #                                'acc' : all_acc\n    #                               }, \n    #                               ignore_index=True\n    #                              )\n    del all_pool, pred, valid_pred\n    _ = gc.collect()\n    \nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\n#result_sum = result_sum.append({'model' : \"CateBoost\", \n#                                'preprocessing':\"huseyincot_all_feat\", \n#                                'name' : \"CTB_huseyincot_all_feat\", \n#                                'y_pred' : oof, \n#                                'acc': acc\n#                               },\n#                               ignore_index=True\n#                              )\nprint('OVERALL CV Kaggle Metric =',acc)\n\ndel oof, acc \n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train \n_=gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T19:33:49.637387Z","iopub.execute_input":"2022-08-24T19:33:49.638242Z","iopub.status.idle":"2022-08-24T19:33:49.846076Z","shell.execute_reply.started":"2022-08-24T19:33:49.638203Z","shell.execute_reply":"2022-08-24T19:33:49.844934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All Model should be download in order to be used in another notebook to create the submission file.","metadata":{}}]}